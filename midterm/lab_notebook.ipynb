{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hailun Zhu       ID: hailunz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "\n",
    "This project is to predict ocean health, one plankton at a time. Details about the requirement can be seen at [Kaggle Link](https://www.kaggle.com/c/datasciencebowl).\n",
    "\n",
    "There are total 121 classes of plankton of the training data. I only used the training data. I select approximately 80% of the training set as my training data, 20% as the test data. Training set: 24269, test set: 6067.\n",
    "\n",
    "\n",
    "## Feature Extraction \n",
    "\n",
    "I select features step by step. All the new feature is tested based on the prevous ones. For this part I use Random Forest classifier provided by sklean, decision tree number 100.\n",
    "\n",
    "#### Rescale image \n",
    "* \n",
    "    * Hypotheses  \n",
    "    The information from the original image could provide inofrmation to help classification.\n",
    "        \n",
    "    * Feature  \n",
    "    Because the image may have different size and the original image could be too large to use, I rescale the image to 25 x 25 size and transfer it to a 1d array of size 25 x 25.\n",
    "    \n",
    "    * Method   \n",
    "    I first read in the image from the folder and then using resize to fix the image size of 25 x 25. Then I use np.reshape to make a 1d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the images and create the features (in readingImage methods)\n",
    "    nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)\n",
    "    image = imread(nameFileImage, as_grey=True)\n",
    "    \n",
    "    image = resize(image, (maxPixel, maxPixel))\n",
    "    \n",
    "    # Store the rescaled image pixels and the axis ratio\n",
    "    X[i, 0:imageSize] = np.reshape(image, (1, imageSize)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  * Result:   \n",
    "    Accuracy of all classes(cross validation): 0.451280580405\n",
    "    \n",
    "    Accuracy of the test: 0.460853799242\n",
    "    \n",
    "    Running time: 856.844261885s\n",
    "    \n",
    "    \n",
    "   * Optimazation  \n",
    "    \n",
    "    I tried to increase the size of the image to see whether the size of the rescale image will affect the result. \n",
    "    I changed the size of the image to 50 X 50.\n",
    "    \n",
    "    The Optimazation results:\n",
    "    \n",
    "    Accuracy of all classes: 0.461760744425\n",
    "    \n",
    "    Accuracy of the test: 0.46958958299\n",
    "    \n",
    "    Running time: 1080.51703596\n",
    "    \n",
    "    \n",
    "  * Conclusion  \n",
    "    Adding the rescale image feature could help classification. Because the it provides the original information about the image. And increasing the size of the rescale image could improve the result because it provides more information about the original image.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Region Properties Feature\n",
    "* \n",
    "    * Hypotheses  \n",
    "    The largest segment is more likely to be the object of interest for classfication.\n",
    "    The information of the largest segment could provide inofrmation to help classification.\n",
    "        \n",
    "    * Feature  \n",
    "    Using measure region properties. I only tested some of the properties:\n",
    "       * Ratio: float  \n",
    "        maxregion.minor_axis_length*1.0 / maxregion.major_axis_length\n",
    "       * filled_area : int  \n",
    "        Number of pixels of filled region\n",
    "       * major_axis_length : float  \n",
    "        The length of the major axis of the ellipse that has the same normalized second central moments as the region.\n",
    "       * minor_axis_length : float  \n",
    "        The length of the minor axis of the ellipse that has the same normalized second central moments as the region.\n",
    "       * perimeter : float  \n",
    "        Perimeter of object which approximates the contour as a line through the centers of border pixels using a 4-connectivity.\n",
    "       * solidity : float  \n",
    "        Ratio of pixels in the region to pixels of the convex hull image.\n",
    "    \n",
    "    * Method   \n",
    "    First I do some preprocess actions on the image, including: thresholding the images, segmenting the images, and extracting region properties. I choose the largest nonzero region.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the largest nonzero region\n",
    "def getLargestRegion(props, labelmap, imagethres):\n",
    "    regionmaxprop = None\n",
    "    for regionprop in props:\n",
    "        # check to see if the region is at least 50% nonzero\n",
    "        if sum(imagethres[labelmap == regionprop.label])*1.0/regionprop.area < 0.50:\n",
    "            continue\n",
    "        if regionmaxprop is None:\n",
    "            regionmaxprop = regionprop\n",
    "        if regionmaxprop.filled_area < regionprop.filled_area:\n",
    "            regionmaxprop = regionprop\n",
    "    return regionmaxprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the largest region, I extract the properties of this region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Feature Ratio\n",
    "def getMinorMajorRatio(image):\n",
    "    image = image.copy()\n",
    "    # Create the thresholded image to eliminate some of the background\n",
    "    imagethr = np.where(image > np.mean(image),0.,1.0)\n",
    "\n",
    "    #Dilate the image\n",
    "    imdilated = morphology.dilation(imagethr, np.ones((4,4)))\n",
    "\n",
    "    # Create the label list\n",
    "    label_list = measure.label(imdilated)\n",
    "    label_list = imagethr*label_list\n",
    "    label_list = label_list.astype(int)\n",
    "\n",
    "    region_list = measure.regionprops(label_list)\n",
    "    maxregion = getLargestRegion(region_list, label_list, imagethr)\n",
    "    \n",
    "     # guard against cases where the segmentation fails by providing zeros\n",
    "    ratio = 0.0\n",
    "    filled_area = 0.0\n",
    "    perimeter = 0.0\n",
    "    solidity = 0.0\n",
    "    if ((not maxregion is None) and  (maxregion.major_axis_length != 0.0)):\n",
    "        ratio = 0.0 if maxregion is None else  maxregion.minor_axis_length*1.0 / maxregion.major_axis_length\n",
    "    filled_area = 0.0 if maxregion is None else maxregion.filled_area\n",
    "    perimeter = 0.0 if maxregion is None else maxregion.perimeter\n",
    "    solidity = 0.0 if maxregion is None else maxregion.solidity\n",
    "\n",
    "    return ratio, filled_area, perimeter, solidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Result:   \n",
    "    1. Only use Ratio property.\n",
    "    \n",
    "    Accuracy of all classes(cross validation): 0.469759054329\n",
    "    \n",
    "    Accuracy of the test: 0.467776495797\n",
    "    \n",
    "    Running time: 1097.83636689\n",
    "    \n",
    "  \n",
    "  * Optimazation  \n",
    "    \n",
    "    I added the filled_area, perimeter and solidity. \n",
    "    \n",
    "    The Optimazation results:\n",
    "    \n",
    "    Accuracy of all classes: 0.480298560882\n",
    "    \n",
    "    Accuracy of the test: 0.481457062799\n",
    "    \n",
    "    Running time: 1060.66653109\n",
    "    \n",
    "    \n",
    "  * Conclusion  \n",
    "    Finding the target interest object area and adding the properties of this area could help improve the feature vector. They provide only the information regarding to the target object. Different plankton may have different target size and  perimeter and solidity.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIFT feature\n",
    "* \n",
    "    * Hypotheses    \n",
    "    Scale-invariant feature transform (or SIFT) is an algorithm in computer vision to detect and describe local features in images. SIFT is rotational invariance. Thus this may be helpful for the classification because the same plankton could be of different orientation. SIFT could help to decrease the effect of the orientation of the image.\n",
    "        \n",
    "    * Feature  \n",
    "    SIFT feature originally is composed of a M*128 array. M is the number of the key points int the image. It varies from image to image.\n",
    "    In order to make the feature to be a constant size 1d array, I use the sum of each column to form a 1*128 1d array.\n",
    "    \n",
    "    * Method  \n",
    "    I use the opencv lib of python to extract the key point descriptor of the SIFT.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get feature sift\n",
    "def getSift(nameFileImage):\n",
    "    img = cv2.imread(nameFileImage)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT()\n",
    "    k, des = sift.detectAndCompute(gray, None)\n",
    "    if des is None:\n",
    "        return [0]*128\n",
    "    else:\n",
    "        res = [0]*128\n",
    "        for d in des:\n",
    "            for i in range(len(d)):\n",
    "                res[i] += d[i]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Result:   \n",
    "    Only use SIFT feature.\n",
    "    \n",
    "    Accuracy of all classes(cross validation): 0.497728131152\n",
    "    \n",
    "    Accuracy of the test: 0.501895500247\n",
    "    \n",
    "    Running time: 1118.5202539\n",
    "    \n",
    "  \n",
    "  * Optimazation  \n",
    "    \n",
    "    I added the SIFT feature key point counts as an additional feature. The hypothese is that the number of the key points also provides information because the SIFT feature does not include this information. \n",
    "    \n",
    "    The Optimazation results:\n",
    "    \n",
    "    Accuracy of all classes: 0.505686500742\n",
    "    \n",
    "    Accuracy of the test: 0.508653370694\n",
    "    \n",
    "    Running time: 1150.40613484\n",
    "    \n",
    "  * Conclusion  \n",
    "  \n",
    "    Adding SIFT and the key point could improve the result. The SIFT rotation-invariant feature helps to improve the result.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get feature sift count\n",
    "def getSiftCount(nameFileImage):\n",
    "    img = cv2.imread(nameFileImage)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT()\n",
    "    k, des = sift.detectAndCompute(gray, None)\n",
    "    if des is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mahotas feature\n",
    "* \n",
    "    * Hypotheses    \n",
    "    There are also some other existing image feature extraction algorithms that could yield good results. I use mahotas library to test some of them.\n",
    "        \n",
    "    * Feature  \n",
    "       * Local binary patterns (LBP) is a type of feature used for classification in computer vision.  \n",
    "       The LBP feature vector, in its simplest form, is created in the following manner:\n",
    "\n",
    "       Divide the examined window into cells (e.g. 16x16 pixels for each cell).\n",
    "       For each pixel in a cell, compare the pixel to each of its 8 neighbors (on its left-top, left-middle, left-bottom, right-top, etc.). Follow the pixels along a circle, i.e. clockwise or counter-clockwise.\n",
    "       Where the center pixel's value is greater than the neighbor's value, write \"1\". Otherwise, write \"0\". This gives an 8-digit binary number (which is usually converted to decimal for convenience).\n",
    "       Compute the histogram, over the cell, of the frequency of each \"number\" occurring (i.e., each combination of which pixels are smaller and which are greater than the center).\n",
    "       Optionally normalize the histogram.\n",
    "       Concatenate (normalized) histograms of all cells. This gives the feature vector for the window.\n",
    "       (From: https://en.wikipedia.org/wiki/Local_binary_patterns)\n",
    "      \n",
    "       * Zernike moments through degree. These are computed on a circle of radius radius centered around cm (or the center of mass of the image, if the cm argument is not used).\n",
    "       Zernike moments are utilized as shape descriptors to classify benign and malignant breast masses.\n",
    "       (From: https://en.wikipedia.org/wiki/Zernike_polynomials)\n",
    "    \n",
    "    * Method  \n",
    "    I use the mahotas lib of python to extract the key point descriptor of the lbp/Zernike moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getMahotasFeature(nameFileImage):\n",
    "    image2 = mh.imread(nameFileImage, as_grey=True)\n",
    "    lbp = mh.features.lbp(image2, radius=20, points=7, ignore_zeros=False)\n",
    "    zernike_moments = mh.features.zernike_moments(image2, radius=20, degree=8)\n",
    "    return lbp, zernike_moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Result:   \n",
    "    1. Only use LBP feature.\n",
    "    \n",
    "    Accuracy of all classes(cross validation): 0.497933914394\n",
    "    \n",
    "    Accuracy of the test: 0.503543761332\n",
    "    \n",
    "    Running time: 1279.74228287\n",
    "   \n",
    "    2. Only use Zernike moments  \n",
    "    \n",
    "    Accuracy of all classes: 0.500700147492\n",
    "    \n",
    "    Accuracy of the test: 0.501521674633\n",
    "    \n",
    "    Running time: 1350.40613484\n",
    "    \n",
    "    \n",
    "  * Conclusion   \n",
    "    Adding LBP/Zernike moments will not improve the results. That maybe because texture information are not important and the information provided by Zernike moments is redundant that is already provided by SIFT. \n",
    "    And as the running time increased by addthing these features, I decide not to include this in the final feature vectors.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of feature extraction\n",
    "\n",
    "    feature        Accuracy of cross validation    Accuracy of the test       running time\n",
    "    rescale_image(25)   0.451280580405                0.460853799242         856.844261885\n",
    "    rescale_image(50)   0.461760744425                0.469589582999         1080.51703596\n",
    "    ratio               0.469759054329                0.467776495797         1097.83636689\n",
    "    ratio+prop          0.480298560882                0.481457062799         1060.66653109\n",
    "    SIFT                0.497728131152                0.501895500247         1118.52025390\n",
    "    SIFT+count          0.505686500742                0.508653370694         1150.40613484\n",
    "    LBP                 0.497933914394                0.503543761332         1279.74228287         Zernike moments     0.500700147492                0.501521674633         1350.40613484"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature vector\n",
    "rescale_image, ratio, filled_area, perimeter, solidity, sift_count, sift\n",
    "\n",
    "## Difficulties in classification(error analysis)\n",
    "\n",
    "1. The features do not include all the information.\n",
    "    Most of the features only provide the shape information of the image and only consider the rotation of the image. For example, I do not use feature to compute holes and sharp curves in the image, which might be an important characteristic of the image. \n",
    "    \n",
    "2. The image has noise.\n",
    "    The noise in the image may result in error.\n",
    "    \n",
    "3. Some of the shapes are difficult to distinguish.\n",
    "    1. Images in same class looks not the same   \n",
    "    acantharia_protist 3721.jpg vs 3733.jpg\n",
    "    \n",
    "    2. Images in different class look similar\n",
    "    Usually, there are some classes that have general similarities, such as acantharia_protist, and acantharia_protist_big_center etc. Only using the shape or outline of the images may misclassify one to another. \n",
    "    For example, the accuracy of the acantharia_protist is 41%, whereas the accuracy of the acantharia_protist_big_center is 0% in the first experiment.\n",
    "    acantharia_protist 337.jpg, acantharia_protist_big_center  29759.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest  \n",
    "    * Hypotheses  \n",
    "    The parameter of the random forest may have influence on the classification result.\n",
    "    \n",
    "    * Method  \n",
    "    Change the number of the decision tree used.\n",
    "    \n",
    "    1. N = 100 (baseline)  \n",
    "    \n",
    "    Accuracy of all classes: 0.505686500742\n",
    "    \n",
    "    Accuracy of the test: 0.508653370694\n",
    "    \n",
    "    Running time: 1150.40613484\n",
    "    \n",
    "    2. N = 200   \n",
    "    \n",
    "    Accuracy of all classes(cross validation): 0.506615930376\n",
    "    \n",
    "    Accuracy of the test: 0.512114718971\n",
    "    \n",
    "    Running time: 1750.24850917\n",
    "    \n",
    "    * Conclusion\n",
    "    Increasing the number of the decision tree used could impove the classification result.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of classifier\n",
    "\n",
    "    feature        Accuracy of cross validation    Accuracy of the test       running time\n",
    "    N=100            0.505686500742                 0.508653370694            1150.40613484\n",
    "    N=200            0.506615930376                 0.512114718971            1750.24850917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The final feature vector is [rescale_image, ratio, filled_area, perimeter, solidity, sift_count, sift], rescle_image size is 50 X 50. Decision tree number is 200. The best accuracy that I got is 51.21%. \n",
    "\n",
    "\n",
    "## Future work\n",
    "\n",
    "1. Adding features that could provide information that focus on edge cases like curves and holes, so that features could cover more cases.\n",
    "\n",
    "2. Using a more state-of-art classifier, or test with other classifier like SVM. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
