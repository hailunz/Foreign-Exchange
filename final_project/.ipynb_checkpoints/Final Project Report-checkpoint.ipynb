{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Final Project Report\n",
    "## Pubmed Central Topic Visualization\n",
    "\n",
    "Hailun Zhu  ID: hailunz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "This project is to produce a visualization of the topics that most commonly co-occur in the pubmed documents with a user provided term or phrase.\n",
    "\n",
    "The project uses the Pubmed Central (PMC) open access dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Document Preparation\n",
    "\n",
    "I assigned each document a unique integer document ID, which is called sessionID in my code. The document id and document filename is stored in the file sessions.csv.\n",
    "\n",
    "### Content Preparation\n",
    "\n",
    "For each document, I select the content of \"body p\" components, based on which I extract the topic of the document.\n",
    "I also applied tokenization, lemmatization and stemming to the raw content and removed the stop words.\n",
    "\n",
    "I use Natural Language Toolkit(nltk) to implement those process.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "In lexical analysis, tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining. Tokenization is useful both in linguistics (where it is a form of text segmentation), and in computer science, where it forms part of lexical analysis[1].\n",
    "\n",
    "The nltk tokenizer divides a string into substrings by splitting on the specified string.\n",
    "\n",
    "#### Stemming and lemmatization\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form[2]. \n",
    "\n",
    "#### Removing stop words\n",
    "\n",
    "Stopwords are high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts.\n",
    "\n",
    "I used nltk English stopwords corpus, including:\n",
    "\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "I also removed words whose length is less than 2.\n",
    "I only keep the alphabetic words and remove numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "I use the count of the words as the feature of a document. That is I use term frequency(tf) feature.\n",
    "I use scikit-learn: machine learning package to help me extract features of each document. \n",
    "\n",
    "Each document will have a sparse feature vector.\n",
    "\n",
    "CountVectorizer converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using scipy.sparse.coo_matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Latent Dirichlet allocation (LDA) is a generative model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. LDA is an example of a topic model[3]. \n",
    "\n",
    "I use LDA to produce a topic model. \n",
    "The baseline of my experiment is to find 10 topics, each of which has 10 words.\n",
    "\n",
    "The produced model has the following variables:\n",
    "\n",
    "* topic_word_:  Point estimate of the topic-word distributions. I use this variable to get the normalized tf of a word in a certain document.\n",
    "\n",
    "* doc_topic_ :  Point estimate of the document-topic distributions. I use this variable to get the relative score of a document to a certain topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Selection Experiment \n",
    "\n",
    "*  Baseline: Topics = 10 , Words = 10\n",
    "\n",
    "    Topic 0: brain concentration plasma animal rat min ecf time quinidine\n",
    "    \n",
    "    Topic 1: concentration sample assay oxytocin level buffer csf ion effect\n",
    "    \n",
    "    Topic 2: model mathrm study concentration value design observed sampling using\n",
    "    \n",
    "    Topic 3: particle formulation protein sample count size flow measurement solution\n",
    "    \n",
    "    Topic 4: patient antibody treatment tumor subject day macitentan exposure expression\n",
    "    \n",
    "    Topic 5: drug peptide transport process direct nanoparticles nose neuropeptides human\n",
    "    \n",
    "    Topic 6: cell vaccine antigen response mouse lns tumor immune eenl\n",
    "    \n",
    "    Topic 7: afm cell drug channel interaction image force therapeutic cytokine\n",
    "    \n",
    "    Topic 8: usepackage model data document method parameter estimate individual missing\n",
    "    \n",
    "    Topic 9: study drug effect based target development potential clinical used\n",
    "    \n",
    "\n",
    "\n",
    "<img src=\"result/images/topics-10-10.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "<h4 align=\"center\">Image1 Topics</h4> \n",
    "\n",
    "\n",
    "<img src=\"result/images/topics-10-10-1.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "<h4 align=\"center\">Image2 Topics-9 details</h4> \n",
    "\n",
    "\n",
    "*  Exp1 : Topics = 20, Words = 10\n",
    "\n",
    "    Topic 0: brain drug transport nose direct human administration nasal model\n",
    "    \n",
    "    Topic 1: usepackage document model end begin minimal amssymb documentclass amsfonts\n",
    "    \n",
    "    Topic 2: macitentan day oxytocin buffer concentration act period ion determined\n",
    "    \n",
    "    Topic 3: cell eenl tumor treated gene tissue mouse cancer expression\n",
    "    \n",
    "    Topic 4: sample concentration csf assay validation atazanavir range method level\n",
    "    \n",
    "    Topic 5: patient model tumor bevacizumab integrin dose expression inhibition clearance\n",
    "    \n",
    "    Topic 6: drug temperature skin rate process sample min freezing crystal\n",
    "    \n",
    "    Topic 7: function product process target nanoparticles approach small component molecule\n",
    "    \n",
    "    Topic 8: brain concentration plasma quinidine animal min ecf csf time\n",
    "    \n",
    "    Topic 9: drug effect potential study clinical vitro safety mechanism therapeutic\n",
    "    \n",
    "    Topic 10: model mathrm concentration value data observed profile telapristone age\n",
    "    \n",
    "    Topic 11: cell antigen lns immune particle response specific vaccine nps\n",
    "    \n",
    "    Topic 12: iron era week hepcidin data vial abt antibody serum\n",
    "    \n",
    "    Topic 13: afm cell force image channel interaction tip panel imaging\n",
    "    \n",
    "    Topic 14: study formulation design sample method used analytical investigation analysis\n",
    "    \n",
    "    Topic 15: fig using result study used time different shown based\n",
    "    \n",
    "    Topic 16: data method covariate missing estimate individual parameter model effect\n",
    "    \n",
    "    Topic 17: vaccine peptide neuropeptides wiv titer response gem immunization vesicle\n",
    "    \n",
    "    Topic 18: particle protein count flow measurement formulation size solution microscopy\n",
    "    \n",
    "    Topic 19: antibody subject positive ligand titer human scoring treatment trial\n",
    "\n",
    "<img src=\"result/images/topics-20-10.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "<h4 align=\"center\">Image3 Topics</h4> \n",
    "\n",
    "\n",
    "*  Exp2 : Topics = 10, Words = 20\n",
    "\n",
    "    Topic 0: brain concentration plasma animal rat min ecf time quinidine csf administration model compartment drug human microdialysis volume unbound acetaminophen\n",
    "    \n",
    "    Topic 1: concentration sample assay oxytocin level buffer csf ion effect week used recovery validation solution stability iron fig using vehicle\n",
    "    \n",
    "    Topic 2: model mathrm study concentration value design observed sampling using time data drug clearance sample profile telapristone age predicted patient\n",
    "    \n",
    "    Topic 3: particle formulation protein sample count size flow measurement solution analytical instrument vial result range analysis preparation era light concentration\n",
    "    \n",
    "    Topic 4: patient antibody treatment tumor subject day macitentan exposure expression concentration period bevacizumab dose growth observed positive baseline integrin inhibition\n",
    "    \n",
    "    Topic 5: drug peptide transport process direct nanoparticles nose neuropeptides human paclitaxel cell protein rate vesicle nasal secretory nanoparticle size delivery\n",
    "    \n",
    "    Topic 6: cell vaccine antigen response mouse lns tumor immune eenl particle specific wiv antibody titer immunization tissue cancer fig gem\n",
    "    \n",
    "    Topic 7: afm cell drug channel interaction image force therapeutic cytokine imaging using panel tip cantilever membrane receptor right tps left\n",
    "    \n",
    "    Topic 8: usepackage model data document method parameter estimate individual missing begin end wasysym amsbsy documentclass amsfonts oddsidemargin upgreek mathrsfs amssymb\n",
    "    \n",
    "    Topic 9: study drug effect based target development potential clinical used factor approach method example test change including use function case\n",
    "\n",
    "\n",
    "<img src=\"result/images/topics-10-20.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "<h4 align=\"center\">Image4 Topics</h4> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Specific Term Topics Experiment\n",
    "\n",
    "I implemented a system to process the PMC dataset with a user-provided term, and generate a JSON file for use in D3-driven visualization.\n",
    "\n",
    "For a single term, I take the normalized tf score in a certain topic as the relative score for the topic to the given score. And then take the top 5 topics based on their scores. Those words in the chosen topics are the words we want to find. They are the most commonly co-occur words for the input term.\n",
    "\n",
    "For a phrase, I simply take the sum of all the words's scores as the score of the phrase. For example, for the phrase 'apple pie', if in topic 1, 'apple' has score 0.5 and 'pie' has score 0.1, then the score for topic1 is 0.5+0.1 = 0.6.\n",
    "Then I take the top 5 topics and their representitive words as the result.\n",
    "\n",
    "\n",
    "* Term: time\n",
    "    Match topic:Topic 0: brain concentration plasma animal rat min ecf time quinidine\n",
    "    \n",
    "    Match topic:Topic 2: model mathrm study concentration value design observed sampling using\n",
    "    \n",
    "    Match topic:Topic 4: patient antibody treatment tumor subject day macitentan exposure expression\n",
    "    \n",
    "    Match topic:Topic 1: concentration sample assay oxytocin level buffer csf ion effect\n",
    "    \n",
    "    Match topic:Topic 3: particle formulation protein sample count size flow measurement solution\n",
    "    \n",
    "    \n",
    " <img src=\"result/images/term-time.png\" height=\"400\" width=\"400\">\n",
    "\n",
    " <h4 align=\"center\">Image5 User term: time</h4> \n",
    "    \n",
    "* Term: vaccine\n",
    "\n",
    "    Match topic:Topic 6: cell vaccine antigen response mouse lns tumor immune eenl\n",
    "\n",
    "    Match topic:Topic 7: afm cell drug channel interaction image force therapeutic cytokine\n",
    "\n",
    "    Match topic:Topic 0: brain concentration plasma animal rat min ecf time quinidine\n",
    "\n",
    "    Match topic:Topic 5: drug peptide transport process direct nanoparticles nose neuropeptides human\n",
    "\n",
    "    Match topic:Topic 3: particle formulation protein sample count size flow measurement solution\n",
    "    \n",
    "    <img src=\"result/images/term-vaccin.png\" height=\"400\" width=\"400\">\n",
    "    \n",
    "    <h4 align=\"center\">Image6 User term: vaccine</h4> \n",
    "\n",
    "* Phrase: time vaccine\n",
    "    \n",
    "    Match topic:Topic 6: cell vaccine antigen response mouse lns tumor immune eenl\n",
    "\n",
    "    Match topic:Topic 0: brain concentration plasma animal rat min ecf time quinidine\n",
    "\n",
    "    Match topic:Topic 2: model mathrm study concentration value design observed sampling using\n",
    "\n",
    "    Match topic:Topic 4: patient antibody treatment tumor subject day macitentan exposure expression\n",
    "\n",
    "    Match topic:Topic 1: concentration sample assay oxytocin level buffer csf ion effect\n",
    "    \n",
    "    <img src=\"result/images/phrase.png\" height=\"400\" width=\"400\">\n",
    "    \n",
    "    <h4 align=\"center\">Image7 User phrase: time vaccine</h4> \n",
    "\n",
    "\n",
    "As can be seen in the above experiment results, some words have dominant topic, some don't. For term 'time', topic0 got the highest score, and it occupies 39.5% in the top 5 topics. For term 'vaccine', topic6 has the higheset score, but it takes over almost 100% among the top 5 topics. This means for term 'vaccine', topic6 is the most relevant and representative topic. For term 'time', we don't have such topics. The reason could be that 'vaccine' is a more special term, 'time' is a more common word. The common term's occurence does not follow a specific pattern, and the special term is exactly opposite. Therefore, the distribution of the special term is more skew, which produces the dominant topic. \n",
    "\n",
    "\n",
    " <img src=\"result/images/time-2.png\" height=\"400\" width=\"400\"> \n",
    " <img src=\"result/images/vaccine.png\" height=\"400\" width=\"400\">\n",
    "    \n",
    " <h4 align=\"center\">Image8 time, vaccine</h4> \n",
    "\n",
    "\n",
    "For the phrase 'time vaccine', we can find that the result is the combination of the result of the two terms. This is just we expected.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization \n",
    "\n",
    "In this section, I tried different visualization methods to draw different graphs based on the baseline experiment parameters. Each visualiztation method use the same input, which is the intermediate data interchange format - flare.json format. \n",
    "\n",
    "* Baseline\n",
    "    topics number: 10\n",
    "    \n",
    "    terms per topic: 10\n",
    "    \n",
    "    Image : \n",
    "    \n",
    "    <img src=\"result/images/term-time.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "    <h4 align=\"center\">Image9 User term: time</h4> \n",
    "    \n",
    "    The relative size of each circle represents the importance of each topic. Could see the detail of words inside a circle. \n",
    "    \n",
    "    \n",
    "* Collapsible Force Layout\n",
    "    <img src=\"result/images/visual1.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "    <h4 align=\"center\">Collapsible Force Layout</h4> \n",
    "    \n",
    "    The size of circle represents score of the term. The relationship bewteen terms, topics and user-given term is more clear.\n",
    "    \n",
    "    \n",
    "* Bubble Chart\n",
    "\n",
    "    <img src=\"result/images/visual2.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "    <h4 align=\"center\">Bubble chart</h4> \n",
    "    \n",
    "    The size of circle represents score of the term. Different colors mean different topics. It is easier to find the term who has the highest score, but less information about the topic.\n",
    "    \n",
    "    \n",
    "* Sunburst Partition\n",
    "    \n",
    "    <img src=\"result/images/visual3.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "    <h4 align=\"center\">Sunburst Partition</h4> \n",
    "\n",
    "    It is more clearly shows the composition of each topic and the relative score of each topic among the 5 topics.\n",
    "    \n",
    "\n",
    "* Summary\n",
    "\n",
    "    The advantages of each topic vary. Different graphs has different focus on what to show. We should choose the visualization method based on requirements and the result we would like to show.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "In this project, I mainly use normalized tf features. This works well. Because term frequency could provide information about the co-occur pattern to some degree. It makes sense that two terms that occurs in the same topic documents are more likely to be related.\n",
    "\n",
    "For a more common term like 'time', the score of the top 5 topics do not differ a lot. This maybe because it occurs everywhere. For term like this could be removed in the process of removing stopwords. Thus, updating the stopwords list could help reduce this case.\n",
    "\n",
    "The tf feature is not enough. I didn't consider idf factor in my project. By applying idf, the term that occurs a lot across the whole corpus should get a small score even though it has high tf. And some of these terms should not be included in stopwords list. Therefore, adding idf factor could help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This project is to produce a visualization of the topics that most commonly co-occur in the pubmed documents with a user provided term or phrase. Term frequency could provide useful information of the topic of the documents. IDF may could help to improve the result. Using different visualization methods could reveal different information of result. Choosing a suitable visualization method according to the information need and requirement is important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [Wiki Tokenization]( https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)\n",
    "\n",
    "[2] [Lemmatization]( http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "[3] [LDA]( https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
